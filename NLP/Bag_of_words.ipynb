{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cadb2065-6be1-475f-930b-a1676e5d2d79",
   "metadata": {},
   "source": [
    "### Introduction: Bag-of-Words (BoW)\n",
    "\n",
    "- Text data is **unstructured** and cannot be directly understood by machines\n",
    "- **Bag-of-Words (BoW)** converts text into a **numerical representation**\n",
    "- It represents text using **word frequency**, ignoring grammar and word order\n",
    "- BoW enables machine learning models to **analyze and predict from text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1db6c3-9bd2-4a8b-a8ff-40fe6c0c937a",
   "metadata": {},
   "source": [
    "#### Example\n",
    "**Data**\n",
    "- Document 1: *\"The cat sat on the mat.\"*\n",
    "- Document 2: *\"The dog chased the cat.\"*\n",
    "\n",
    "**Step 1: Tokenization**\n",
    "- Doc 1 Tokens: `[\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]`\n",
    "- Doc 2 Tokens: `[\"the\", \"dog\", \"chased\", \"the\", \"cat\"]`\n",
    "\n",
    "**Step 2: Vocabulary Creation**\n",
    "- Vocabulary: `{\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"chased\"}`\n",
    "- Vocabulary Size = **7**\n",
    "\n",
    "**Step 3: Word Frequency Count**\n",
    "\n",
    "| Word     | Doc 1 | Doc 2 |\n",
    "|---------|-------|-------|\n",
    "| the     | 2     | 2     |\n",
    "| cat     | 1     | 1     |\n",
    "| sat     | 1     | 0     |\n",
    "| on      | 1     | 0     |\n",
    "| mat     | 1     | 0     |\n",
    "| dog     | 0     | 1     |\n",
    "| chased  | 0     | 1     |\n",
    "\n",
    "**Step 4: Vector Representation**\n",
    "- Vocabulary Order: `[\"the\", \"cat\", \"sat\", \"on\", \"mat\", \"dog\", \"chased\"]`\n",
    "- Doc 1 Vector: `[2, 1, 1, 1, 1, 0, 0]`\n",
    "- Doc 2 Vector: `[2, 1, 0, 0, 0, 1, 1]`\n",
    "\n",
    "**Key Idea**\n",
    "- Word order is ignored â†’ only **frequency matters**\n",
    "- Text is converted into **numerical vectors** usable by ML models\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b00a95-1da9-4182-8932-4ca419364b19",
   "metadata": {},
   "source": [
    "### Process of Vocabulary Creation\n",
    "1. **Gather Documents**: Collect all text documents for analysis\n",
    "2. **Tokenization**:\n",
    "   - Split text into words/tokens.\n",
    "   - Can use whitespace, punctuation, or NLP libraries\n",
    "4. **Normalization**\n",
    "   Convert text to a consistent format:\n",
    "    - Lowercasing words\n",
    "    - Removing punctuation\n",
    "    - Removing numbers (optional)\n",
    "5. **Identify Unique Tokens**\n",
    "  - Collect all normalized tokens\n",
    "  - Unique tokens form the **vocabulary**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11dfc987-3031-4a57-ae8a-537c950f6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b379df-9a19-4ea3-b99e-1f5b315c71ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog chased the cat.\"\n",
    "] # data\n",
    "\n",
    "# Get English stop words and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96281309-cc6d-4563-9214-b8dfcc643237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Corpus:['The cat sat on the mat.', 'The dog chased the cat.']\n",
      "All Tokens:['cat', 'sat', 'mat', 'dog', 'chased', 'cat']\n",
      "Vocabulary:['cat', 'chased', 'dog', 'mat', 'sat']\n"
     ]
    }
   ],
   "source": [
    "all_tokens = []\n",
    "for doc in corpus:\n",
    "    # Tokenize the document\n",
    "    tokens = word_tokenize(doc.lower()) # Convert to lowercase and tokenize\n",
    "\n",
    "    # Remove punctuation and stop words\n",
    "    filtered_tokens = [\n",
    "        word for word in tokens\n",
    "        if word not in stop_words and word not in punctuation\n",
    "    ]\n",
    "    all_tokens.extend(filtered_tokens)\n",
    "\n",
    "# Create the vocabulary (set of unique tokens)\n",
    "vocabulary = sorted(list(set(all_tokens)))\n",
    "print(f\"Original Corpus:{corpus}\")\n",
    "print(f\"All Tokens:{all_tokens}\") #(after lowercasing, tokenization, stop word & punctuation removal)\n",
    "print(f\"Vocabulary:{vocabulary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191781ce-bb63-4b7b-ad6c-2093a9a3e61a",
   "metadata": {},
   "source": [
    "## Document-Term Matrix: Numerical Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbfee32-f314-4289-85f2-e0d42255153f",
   "metadata": {},
   "source": [
    "It is used as the core numerical representation in **Bag-of-Words**. It enables machine learning models to process text data\n",
    "\n",
    "- **Rows** represent **documents**\n",
    "- **Columns** represent **terms (vocabulary words)**\n",
    "- **Cell values** show **frequency of a term in a document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65c9961c-f5eb-49a0-b1ab-f819e810b59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-Term Matrix:\n",
      "[[1 0 0 1 1]\n",
      " [1 1 1 0 0]]\n",
      "Vocabulary (column headers):\n",
      "['cat', 'chased', 'dog', 'mat', 'sat']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# taking the previous result vocabulary\n",
    "# Word counts for Document 1 (based on vocabulary order)\n",
    "doc1_counts = [1, 0, 0, 1, 1]\n",
    "\n",
    "# Word counts for Document 2 (based on vocabulary order)\n",
    "doc2_counts = [1, 1, 1, 0, 0]\n",
    "\n",
    "# Create the Document-Term Matrix\n",
    "document_term_matrix = np.array([doc1_counts, doc2_counts])\n",
    "\n",
    "print(\"Document-Term Matrix:\")\n",
    "print(document_term_matrix)\n",
    "print(\"Vocabulary (column headers):\")\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a8e40f-c045-4216-b94e-ea499534c22d",
   "metadata": {},
   "source": [
    "### Implementing Bag-of-Words with Scikit-learn's CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f994d63b-f1d2-4820-a6e6-d5e53d9408a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a23afc40-9754-4c4a-9594-f4368ba12d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "    \"This is a sample document for demonstration purposes.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3e20e77-2501-4bf2-aad8-60add83ae5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the resulting DTM: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Shape of the DTM (n_documents, n_features): (5, 13)\n",
      "Vocabulary (Features): ['and' 'demonstration' 'document' 'first' 'for' 'is' 'one' 'purposes'\n",
      " 'sample' 'second' 'the' 'third' 'this']\n",
      "Document-Term Matrix (as Pandas DataFrame):\n",
      "       and  demonstration  document  first  for  is  one  purposes  sample  \\\n",
      "Doc 0    0              0         1      1    0   1    0         0       0   \n",
      "Doc 1    0              0         2      0    0   1    0         0       0   \n",
      "Doc 2    1              0         0      0    0   1    1         0       0   \n",
      "Doc 3    0              0         1      1    0   1    0         0       0   \n",
      "Doc 4    0              1         1      0    1   1    0         1       1   \n",
      "\n",
      "       second  the  third  this  \n",
      "Doc 0       0    1      0     1  \n",
      "Doc 1       1    1      0     1  \n",
      "Doc 2       0    1      1     1  \n",
      "Doc 3       0    1      0     1  \n",
      "Doc 4       0    0      0     1  \n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "#Fit the vectorizer to the corpus and transform the corpus into a DTM\n",
    "# The fit_transform method does two things:\n",
    "# a) It learns the vocabulary from the corpus (fit).\n",
    "# b) It converts the corpus into a Document-Term Matrix (transform).\n",
    "dtm = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Type of the resulting DTM:\", type(dtm))\n",
    "print(\"Shape of the DTM (n_documents, n_features):\", dtm.shape)\n",
    "\n",
    "# To see the vocabulary (the features/column names)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Vocabulary (Features):\", feature_names)\n",
    "\n",
    "# To view the DTM as a dense Pandas DataFrame for better readability\n",
    "# Note: Converting large sparse matrices to dense can consume a lot of memory.\n",
    "# For demonstration with a small corpus, it's fine.\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns=feature_names)\n",
    "dtm_df.index.name = 'Document Index'\n",
    "dtm_df.index = [f'Doc {i}' for i in range(len(corpus))]\n",
    "\n",
    "print(\"Document-Term Matrix (as Pandas DataFrame):\")\n",
    "print(dtm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5744ad-b888-443e-a057-7f3358b0331f",
   "metadata": {},
   "source": [
    "## Understanding Sparsity in Document-Term Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4ccd7e-be98-47ae-8a21-fba8deab6006",
   "metadata": {},
   "source": [
    "#### What Causes Sparsity in BoW?\n",
    "- **Large Vocabulary**: Thousands or millions of unique words\n",
    "- **Short Documents**: Each document uses only a small subset of the vocabulary\n",
    "- **Uneven Word Distribution**: Many words appear in very few documents\n",
    "\n",
    "#### Why is Sparsity Important?\n",
    "**1. Computational Efficiency**\n",
    "- Saves memory using sparse matrix formats (CSR, CSC)\n",
    "- Speeds up computations by ignoring zero values\n",
    "\n",
    "**2. Algorithmic Implications**\n",
    "- Helps identify informative features\n",
    "- Some algorithms (e.g., KNN) struggle with high sparsity\n",
    "\n",
    "**3. Dimensionality Reduction**\n",
    "- Indicates need for PCA, SVD, etc.\n",
    "- Improves performance of certain models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ccae2e1-0600-48f6-9c18-786d553939a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero elements: 28\n"
     ]
    }
   ],
   "source": [
    "# Finding sparsity\n",
    "print(f\"Number of non-zero elements: {dtm.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fee20da-fd80-4084-aaaf-ab2b17871cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the DTM sparse? True\n",
      "Is the dense version sparse? False\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import issparse\n",
    "# To convert to a dense array (use with caution for large matrices)\n",
    "dtm_dense = dtm.toarray()\n",
    "print(\"Is the DTM sparse?\", dtm.getformat() == 'csr') # CSR is a sparse format\n",
    "print(\"Is the dense version sparse?\", issparse(dtm_dense)) # False, it's a numpy array\n",
    "# NumPy arrays(dtm_dense) do not have getformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b30cd-3fa9-4e74-957b-914543b36e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
