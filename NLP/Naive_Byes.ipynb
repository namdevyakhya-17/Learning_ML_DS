{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83a7405-3dec-4171-b9d7-497da0dcfbea",
   "metadata": {},
   "source": [
    "1. **Gaussian Naive Bayes** : Gaussian Naive Bayes assumes that the features follow a Gaussian (normal) distribution. This variant is typically used for continuous features.\n",
    " - **Assumption**: The probability of a feature value, given a class, is drawn from a Gaussian distribution.\n",
    " - **When to Use**: When your features are continuous and can be reasonably approximated by a normal distribution. Examples include height, weight, temperature, or pixel intensity values in image classification.\n",
    "2. **Multinomial Naive Bayes**: Multinomial Naive Bayes is designed for discrete features that represent counts or frequencies. This is the most common variant used for text classification.\n",
    " - **Assumption**: The probability of observing a feature (word) given a class follows a multinomial distribution. This distribution is suitable for modeling counts of events (like word occurrences) in a fixed number of trials (like the total number of words in a document).\n",
    " - **When to Use**: This is the go-to variant for text classification tasks where features represent word counts, term frequencies, or TF-IDF scores.\n",
    "3. **Bernoulli Naive Bayes**: Bernoulli Naive Bayes is also for discrete features, but it models the presence or absence of a feature, rather than its frequency. It assumes that each feature is a binary variable.\n",
    " - **Assumption**: Each feature (word) is either present or absent in a document. The probability of a feature being present or absent is modeled.\n",
    " - **When to Use**: When your features are binary, indicating presence or absence. This can be useful for text if you are only considering whether a word appears in a document, not how many times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ed6803-516e-4b4a-82f2-6ff7f18f5fd4",
   "metadata": {},
   "source": [
    "## Why Multinomial Naive Bayes Reigns Supreme for Text Classification\n",
    "Multinomial Naive Bayes (MNB) is one of the most effective and widely used algorithms for text classification tasks in Natural Language Processing (NLP).\n",
    "\n",
    "### 1. Specifically Designed for Text Data\n",
    "Text data is naturally represented using word counts or frequencies (Bag-of-Words, Term Frequency, TF-IDF).\n",
    "Multinomial Naive Bayes directly models these word frequency distributions, making it ideal for text-based problems.\n",
    "\n",
    "### 2. Handles High-Dimensional and Sparse Data Well\n",
    "Text datasets typically contain:\n",
    "- A very large vocabulary (thousands of features)\n",
    "- Sparse feature vectors (mostly zeros)\n",
    "\n",
    "MNB performs efficiently in such high-dimensional sparse spaces.\n",
    "\n",
    "### 3. Fast Training and Prediction\n",
    "- Training involves simple counting of word occurrences\n",
    "- Prediction is based on probability calculations\n",
    "\n",
    "This makes MNB extremely fast and suitable for large-scale datasets.\n",
    "\n",
    "### 4. Effective Despite the Naive Independence Assumption\n",
    "MNB assumes that words occur independently of each other, which is not true in real language.\n",
    "However, this assumption works surprisingly well in practice and still produces strong results.\n",
    "\n",
    "### 5. Performs Well with Small Datasets\n",
    "Unlike deep learning models, MNB does not require large amounts of training data.\n",
    "It generalizes well even with limited labeled text data.\n",
    "\n",
    "### 6. Robust to Rare and Unseen Words (Using Smoothing)\n",
    "Laplace (Additive) Smoothing:\n",
    "- Prevents zero probabilities\n",
    "- Allows the model to handle unseen words during testing\n",
    "\n",
    "This is essential for real-world text applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "625a770f-ffa4-454a-8167-1dc62d7ad6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5af41728-182e-4738-bcd1-a0f019025f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of clicking (P(Click)): 0.2600\n"
     ]
    }
   ],
   "source": [
    "# Defining prior probabilities\n",
    "p_purchase=0.1\n",
    "p_no_purchase=0.9\n",
    "\n",
    "# Likelihoods\n",
    "p_click_given_purchase = 0.8\n",
    "p_click_given_no_purchase = 0.2\n",
    "\n",
    "# Calculate the probability of the evidence (clicking)\n",
    "# P(Click) = P(Click | Purchase) * P(Purchase) + P(Click | No Purchase) * P(No Purchase)\n",
    "p_click = (p_click_given_purchase * p_purchase) + (p_click_given_no_purchase * p_no_purchase)\n",
    "print(f\"Probability of clicking (P(Click)): {p_click:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac194993-9a87-4548-a97a-3997e98bab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Purchase given Click (P(Purchase | Click)): 0.3077\n"
     ]
    }
   ],
   "source": [
    "# Calculate the posterior probability P(Purchase | Click) using Bayes' Theorem\n",
    "# P(Purchase | Click) = (P(Click | Purchase) * P(Purchase)) / P(Click)\n",
    "p_purchase_given_click = (p_click_given_purchase * p_purchase) / p_click\n",
    "\n",
    "print(f\"Probability of Purchase given Click (P(Purchase | Click)): {p_purchase_given_click:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b93fe022-2fa1-45d1-81dc-b448151de0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of No Purchase given Click (P(No Purchase | Click)): 0.6923\n",
      "Sum of posterior probabilities: 1.0000\n"
     ]
    }
   ],
   "source": [
    "p_no_purchase_given_click = (p_click_given_no_purchase * p_no_purchase) / p_click\n",
    "print(f\"Probability of No Purchase given Click (P(No Purchase | Click)): {p_no_purchase_given_click:.4f}\")\n",
    "\n",
    "# Verify that the posterior probabilities sum to 1\n",
    "print(f\"Sum of posterior probabilities: {p_purchase_given_click + p_no_purchase_given_click:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1175c-7858-4e8c-b729-3d51c0cdb47d",
   "metadata": {},
   "source": [
    "## Implementation of Multinomial bayes for text processing and trainign it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99a5d3e4-ce64-4877-b949-3b828121d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80a0d176-586b-49b4-812e-1cf531795c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label\n",
      "0  This is the first document. It is about machin...    ML\n",
      "1  This document is the second document. It discu...    AI\n",
      "2  And this is the third one. Machine learning is...    ML\n",
      "3  Is this the first document again? Data science...    ML\n",
      "4  Artificial intelligence and machine learning a...    AI\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset\n",
    "data = {\n",
    "    'text': [\n",
    "        'This is the first document. It is about machine learning and data science.',\n",
    "        'This document is the second document. It discusses artificial intelligence.',\n",
    "        'And this is the third one. Machine learning is fun!',\n",
    "        'Is this the first document again? Data science is important.',\n",
    "        'Artificial intelligence and machine learning are related fields.',\n",
    "        'The stock market experienced a significant downturn today.',\n",
    "        'New advancements in AI are revolutionizing healthcare.',\n",
    "        'Learning Python for data analysis is highly recommended.',\n",
    "        'The weather forecast predicts rain for tomorrow.',\n",
    "        'Natural Language Processing is a subfield of AI.'\n",
    "    ],\n",
    "    'label': ['ML', 'AI', 'ML', 'ML', 'AI', 'Finance', 'AI', 'ML', 'Weather', 'AI']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0388d428-d4d6-4ad5-a8a1-6b31175e0a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after preprocessing:\n",
      "                                                text label processed_text\n",
      "0  This is the first document. It is about machin...    ML          sssss\n",
      "1  This document is the second document. It discu...    AI        sssssss\n",
      "2  And this is the third one. Machine learning is...    ML            sss\n",
      "3  Is this the first document again? Data science...    ML          sssss\n",
      "4  Artificial intelligence and machine learning a...    AI               \n"
     ]
    }
   ],
   "source": [
    "# Get English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() # Lowercasing\n",
    "    text = re.sub(r'[^\\\\w\\\\s]', '', text) # Remove punctuation\n",
    "    tokens = text.split() # Tokenize and remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"DataFrame after preprocessing:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f957ad0-59e8-4ba3-81b1-dba3879819bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ML' 'ML' 'ML']\n",
      "Model Accuracy: 0.00\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "          AI       0.00      0.00      0.00       1.0\n",
      "     Finance       0.00      0.00      0.00       1.0\n",
      "          ML       0.00      0.00      0.00       0.0\n",
      "     Weather       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       3.0\n",
      "   macro avg       0.00      0.00      0.00       3.0\n",
      "weighted avg       0.00      0.00      0.00       3.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\envs\\ml_ds_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\ml_ds_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\ml_ds_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\ml_ds_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\ml_ds_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\ml_ds_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['processed_text'])\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Training the model that has already been preprocessed\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(X_train, y_train)\n",
    "y_pred = mnb_model.predict(X_test)\n",
    "print(y_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc5c6d-d560-4f8f-b4c2-93ca61e2d9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
