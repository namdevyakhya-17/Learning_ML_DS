{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba72dc37-d6b9-4083-9c5b-5e6388b413c8",
   "metadata": {},
   "source": [
    "### Why Text Data is Challenging for Machines\n",
    "\n",
    "- **Ambiguity & Polysemy**: Same word, multiple meanings (e.g., *bank*)\n",
    "\n",
    "- **Synonyms & Paraphrasing**: Different words/phrases can mean the same thing\n",
    "\n",
    "- **Context Dependency**: Meaning changes based on surrounding text\n",
    "\n",
    "- **Language Variations**: Slang, misspellings, abbreviations, informal usage\n",
    "\n",
    "- **Sentiment & Tone**: Sarcasm and irony are hard to detect\n",
    "\n",
    "- **Structure & Syntax**: Complex or broken grammar in real-world text\n",
    "\n",
    "- **Data Volume & Noise**:  Large datasets with irrelevant or noisy content\n",
    "\n",
    "- **Lack of Numerical Form**: Text must be converted into numbers for ML models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728d30b3-003d-4b2c-92fa-84d63530070a",
   "metadata": {},
   "source": [
    "### Why Text Cleaning is Crucial\n",
    "1. **Reduces Noise**: Removes punctuation and symbols that add little semantic value\n",
    "2. **Avoids Artificial Differences**: Treats words like `apple` and `apple.` as the same token\n",
    "3. **Improves Frequency Counts**: Prevents numbers and symbols from inflating vocabulary size\n",
    "4. **Boosts Model Performance**: Helps models focus on meaningful patterns\n",
    "5. **Lowers Computational Cost**: Smaller, cleaner data → faster training and less memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5321ebbe-3d02-4d7d-a947-47dbd8f21745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a sample sentence with punctuation it costs and has items awesome\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re  # for regular expression\n",
    "\n",
    "def clean_text(text):\n",
    "    text=text.lower()   # convert to lowercase\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation\n",
    "    text = ''.join([char for char in text if not char.isdigit()])  # Remove numbers\n",
    "    text = ' '.join(text.split())    # Remove speacial character\n",
    "    return text \n",
    "sample_text = \"This is a sample sentence, with punctuation! It costs $19.99 and has 2 items. @Awesome!\"\n",
    "cleaned_sample = clean_text(sample_text)\n",
    "print(cleaned_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b10efc-f7ad-428c-87c9-e36bd8f3e65c",
   "metadata": {},
   "source": [
    "### Breaking Down Text: The Power of Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e10dc-6438-452d-b826-4a5d1a789b4f",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting text into smaller units called **tokens**. Tokens can be **words, punctuation, or sub-word units**.          \n",
    "**Types of Tokenization**\n",
    "1. **Word Tokenization**\n",
    "   - Splits text into individual words\n",
    "   - Uses spaces and punctuation as delimiters\n",
    "   - Example:  \n",
    "     `\"NLP is fascinating!\"` → `[\"NLP\", \"is\", \"fascinating\", \"!\"]`\n",
    "\n",
    "2. **Sentence Tokenization**\n",
    "   - Splits text into individual sentences\n",
    "   - Uses `.`, `?`, `!` as delimiters\n",
    "   - Each sentence is treated as a separate unit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb65802-48fa-441f-b09f-b6c4b28aec64",
   "metadata": {},
   "source": [
    "#### Why Tokenization is Important\n",
    "- **Enables Feature Extraction**: Converts text into units usable for BoW, TF-IDF, etc.\n",
    "- **Reduces Complexity**: Breaks text into manageable pieces\n",
    "- **Supports Linguistic Analysis**: First step for POS tagging, NER, parsing\n",
    "- **Standardizes Input**: Ensures consistent text processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "571b5ef1-e1ec-48e4-9766-161b61403ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk  # provides function to tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4277abcc-eabb-49b7-82bb-2405c8b62ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'a', 'fundamental', 'step', 'in', 'NLP', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# word tokenize\n",
    "text_to_tokenize = \"Tokenization is a fundamental step in NLP!\"\n",
    "word_tokens = word_tokenize(text_to_tokenize)\n",
    "print(word_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b371cb6-4c4f-42a4-8652-8770e50d5406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing is fascinating.', 'It allows computers to understand human language.', 'This is a complex but rewarding field!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "# sentence tokenize\n",
    "paragraph = \"Natural Language Processing is fascinating. It allows computers to understand human language. This is a complex but rewarding field!\"\n",
    "sentence_tokens = sent_tokenize(paragraph)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44056bb-cffa-4ee8-97ec-4af746cac07b",
   "metadata": {},
   "source": [
    "### Filtering the Noise: The Role of Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b3d65-7b4c-4357-9fc2-4097cee35b36",
   "metadata": {},
   "source": [
    "These words, such as \"the,\" \"a,\" \"is,\" \"in,\" \"and,\" \"of,\" carry little semantic meaning on their own and can dominate the feature space in our models, potentially obscuring more important words. These common words are known as stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e8d03-3894-446d-b057-f186078bcbbd",
   "metadata": {},
   "source": [
    "#### Why Remove Stop Words?\n",
    "1. **Reduces Dimensionality**\n",
    "  - Decreases vocabulary size\n",
    "  - Faster training and lower memory usage\n",
    "2. **Improves Model Performance**\n",
    "  - Removes less informative words\n",
    "  - Helps models focus on meaningful terms (e.g., *good*, *bad*)\n",
    "3. **Enhances Interpretability**\n",
    "  - Highlights important words in analysis and visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "109831dc-0936-4e0c-9e63-e0a3a72f690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "695fa253-40de-4497-beb1-de49adf7eb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['This', 'is', 'a', 'sample', 'sentence', 'to', 'demonstrate', 'stop', 'word', 'removal', '.']\n",
      "Filtered tokens: ['sample', 'sentence', 'demonstrate', 'stop', 'word', 'removal', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Get the set of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sentence = \"This is a sample sentence to demonstrate stop word removal.\"\n",
    "word_tokens = word_tokenize(sentence)\n",
    "# Remove stop words\n",
    "filtered_sentence = [w for w in word_tokens if w.lower() not in stop_words]\n",
    "\n",
    "print(\"Original tokens:\", word_tokens)\n",
    "print(\"Filtered tokens:\", filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8dc6d-9d9f-46a6-a908-b3c69e649081",
   "metadata": {},
   "source": [
    "### Reducing Words to Their Roots: Stemming vs. Lemmatization\n",
    "\n",
    "In natural language, the same concept can appear in multiple word forms. Words like *run, running, ran,* and *runs* all convey the same core meaning, but machine learning models treat them as separate tokens unless normalization is applied. This increases vocabulary size and weakens the importance of the underlying concept. To address this, **stemming** and **lemmatization** are used to reduce words to their base or root form.\n",
    "\n",
    "A major challenge is **inflectional variation**. Words such as *play, playing, played* or *computer, computing, computed* are semantically related but appear different to a model. Without normalization, the model considers them unrelated. Stemming and lemmatization map such variations to a common base form, helping models learn more effectively.\n",
    "\n",
    "1. **Stemming** is a heuristic process that removes prefixes or suffixes from words to obtain a stem. It focuses on speed rather than linguistic accuracy and often produces stems that are not valid English words. For example, *studies* may become *studi*. Stemming is rule-based, fast, and computationally inexpensive, but it can be inaccurate due to over-stemming or under-stemming. Common stemming algorithms include the Porter Stemmer, Snowball Stemmer, and Lancaster Stemmer, with Lancaster being the most aggressive.\n",
    "\n",
    "2. **Lemmatization**, on the other hand, is a more advanced technique that uses vocabulary and morphological analysis to return the dictionary form of a word, known as the lemma. Unlike stemming, lemmatization produces valid English words and can consider the part of speech to determine the correct base form. For example, the word *better* is correctly lemmatized to *good*. Although lemmatization is slower and requires lexical resources, it is more accurate and linguistically meaningful.\n",
    "\n",
    "#### Example:\n",
    "Consider the word \"better.\"\n",
    "**Stemming**: Might reduce it to \"better\" or \"bett.\"                                                                                                 \n",
    "**Lemmatization**: Will correctly identify it as the comparative form of \"good\" and return \"good.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "716cd76b-e76e-418a-bfe9-60207d2dbc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1f64751-73e4-4d1c-9c05-1390a3de3fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['running', 'runs', 'ran', 'easily', 'fairly', 'studies', 'studying', 'computation', 'computational', 'computer', 'better']\n",
      "Stemmed Words: ['run', 'run', 'ran', 'easili', 'fairli', 'studi', 'studi', 'comput', 'comput', 'comput', 'better']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "#Stemming\n",
    "porter = PorterStemmer()\n",
    "words_to_stem = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\", \"studies\", \"studying\", \"computation\", \"computational\", \"computer\", \"better\"]\n",
    "stemmed_words = [porter.stem(word) for word in words_to_stem]\n",
    "\n",
    "print(\"Original Words:\", words_to_stem)\n",
    "print(\"Stemmed Words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46127390-00fd-4e0d-a0f1-f9d8a58f0957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas (no POS): ['running', 'run', 'ran', 'easily', 'fairly', 'study', 'studying', 'computation', 'computational', 'computer', 'better']\n",
      "Lemma for 'better' as adjective: good\n",
      "Lemma for 'studies' as verb: study\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas_no_pos = [lemmatizer.lemmatize(word) for word in words_to_stem]\n",
    "print(\"Lemmas (no POS):\", lemmas_no_pos)\n",
    "\n",
    "# Note: NLTK's lemmatizer uses specific POS tags like 'a' for adjective, 'r' for adverb, 'v' for verb, 'n' for noun.\n",
    "lemma_better_adj = lemmatizer.lemmatize('better', pos='a')\n",
    "print(f\"Lemma for 'better' as adjective: {lemma_better_adj}\")\n",
    "\n",
    "# Example for 'studies' as a verb\n",
    "lemma_studies_verb = lemmatizer.lemmatize('studies', pos='v')\n",
    "print(f\"Lemma for 'studies' as verb: {lemma_studies_verb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa9bbc0-8bf2-4a72-988c-2684ba944579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
