{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df70a1b1-eee1-49e4-ab3a-c61bce059511",
   "metadata": {},
   "source": [
    "# Scikit-learn Estimator API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b1fdd9-5677-4156-a69a-71af24185823",
   "metadata": {},
   "source": [
    "# The 'fit' Method: Learning from Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927e1d2-45d3-41ff-8055-25687aa4bdb2",
   "metadata": {},
   "source": [
    "The **fit** method is where the learning happens. When you call fit on an estimator, you provide it with your training data. The estimator then analyzes this data to learn patterns, relationships, and parameters that will be used for making predictions or transformations later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe557b8-3032-4534-87f6-441826a21df5",
   "metadata": {},
   "source": [
    "**Syntax**:- estimator.fit(X_train, y_train)\n",
    "\n",
    "Here:\n",
    "\n",
    "**estimator**: An instance of a Scikit-learn model (e.g., LinearRegression(), LogisticRegression(), KMeans()).                                      \n",
    "**X_train**: The training feature data, typically a NumPy array or Pandas DataFrame.                                                                 \n",
    "**y_train**: The training target data (labels or values), also typically a NumPy array or Pandas Series. For unsupervised learning algorithms, y_train is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a721af1c-135f-4a2d-ba27-dbf18cc0741f",
   "metadata": {},
   "source": [
    "# The 'predict' Method: Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec5c7d1-df30-459c-b08c-15df0c1791bb",
   "metadata": {},
   "source": [
    "Once an estimator has been fitted to the training data, it can be used to make predictions on new, unseen data. This is achieved using the predict method.\n",
    "\n",
    "**Syntax**:- predictions = estimator.predict(X_test)                                                                                                \n",
    "**X_test**: The feature data for which you want to make predictions. This should have the same structure and features as the data used for fitting, but it contains new, unseen samples.                                                                                                                 \n",
    "**predictions**: The output of the method, which will be an array of predicted values or class labels, corresponding to each sample in X_test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d0d852-c241-42ce-9126-07e6a3657cbc",
   "metadata": {},
   "source": [
    "#  The 'transform' Method: Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ce931-b3c8-42cf-b0d0-7020c6b63018",
   "metadata": {},
   "source": [
    "While fit and predict are central to model building, many Scikit-learn objects, particularly those in the sklearn.preprocessing module, use the transform method. These objects are often called transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857cf008-8741-48f1-90e7-978fbb1f7844",
   "metadata": {},
   "source": [
    "Transformers often have a fit_transform method, which combines fitting the transformer to the data and then transforming the data in one step. Alternatively, you can call fit and then transform separately.                                                                                     \n",
    "\n",
    "#### Using fit_transform\n",
    "transformer = StandardScaler()                                                                                                                      \n",
    "X_scaled = transformer.fit_transform(X_train)\n",
    "\n",
    "#### Using fit and transform separately\n",
    "transformer = StandardScaler()                                                                                                                      \n",
    "transformer.fit(X_train)                                                                                                                           \n",
    "X_scaled = transformer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d93b8c-deb7-4a41-b2e6-96d9022f5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb04689-6c16-44ad-a6a8-f5a625f13f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'feature1': np.random.rand(100) * 100,\n",
    "        'feature2': np.random.rand(100) * 1000,\n",
    "        'target': np.random.rand(100) * 50}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df[['feature1', 'feature2']]\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f7c037-f8ba-4dee-9728-9acc39890770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X_train shape: (80, 2)\n",
      "Scaled X_train shape: (80, 2)\n",
      "Original X_test shape: (20, 2)\n",
      "Scaled X_test shape: (20, 2)\n",
      "Sample of scaled X_train:  [[ 0.75759118 -0.10651748]\n",
      " [ 0.12090698 -1.27390205]\n",
      " [-1.23829021  0.06255202]\n",
      " [ 1.48969527  0.00477849]\n",
      " [ 1.67021941  1.16041632]]\n",
      "Sample of scaled X_test:  [[-0.84839566 -0.15582781]\n",
      " [ 0.4496746   0.29350611]\n",
      " [-1.50850036 -0.62152603]\n",
      " [ 1.28451798  0.63299618]\n",
      " [-0.44825264 -0.34558095]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "# Instantiate and fit_transform on training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) # Uses learned parameters to transform data\n",
    "\n",
    "# Transform test data using the SAME scaler fitted on training data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Original X_train shape:\", X_train.shape)\n",
    "print(\"Scaled X_train shape:\", X_train_scaled.shape)\n",
    "print(\"Original X_test shape:\", X_test.shape)\n",
    "print(\"Scaled X_test shape:\", X_test_scaled.shape)\n",
    "print(\"Sample of scaled X_train: \", X_train_scaled[:5])\n",
    "print(\"Sample of scaled X_test: \", X_test_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b105fcbf-7c50-44bc-aa4e-11f56a505e30",
   "metadata": {},
   "source": [
    "# Structuring Your Data: Feature Matrices (X) and Target Vectors (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50806306-1365-460b-93ff-0e2b44229e88",
   "metadata": {},
   "source": [
    "The feature matrix, X, is a 2-dimensional array-like structure (typically a NumPy array or a Pandas DataFrame) where each row represents a single observation or sample, and each column represents a specific feature or attribute of that observation. These features are the independent variables that your model will use to learn and make predictions.\n",
    "\n",
    "##### Why is it important?\n",
    "\n",
    "Machine learning models learn by identifying patterns and relationships between features and the target variable. The structure of X ensures that the model receives consistent information for each observation. Each feature column should contain numerical data (or be appropriately encoded if it's categorical) and should be relevant to the problem you are trying to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37bf1242-e926-4d6d-86d4-473a34dc31ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix (X):\n",
      "   Tenure  MonthlyCharges  TotalCharges    ContractType InternetService\n",
      "0       9          104.36       6045.15        Two year             DSL\n",
      "1       4           75.65       4970.96        Two year     Fiber optic\n",
      "2      69          102.31       3188.17  Month-to-month     Fiber optic\n",
      "3      36          116.65       1031.48        One year             DSL\n",
      "4      12           54.96       5960.68        Two year              No\n",
      "Shape of X: (10, 5)\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'CustomerID': [f'CUST{i:04d}' for i in range(1, 11)],\n",
    "    'Tenure': np.random.randint(1, 72, 10),\n",
    "    'MonthlyCharges': np.random.uniform(20, 120, 10).round(2),\n",
    "    'TotalCharges': np.random.uniform(20, 8000, 10).round(2),\n",
    "    'ContractType': np.random.choice(['Month-to-month', 'One year', 'Two year'], 10),\n",
    "    'InternetService': np.random.choice(['DSL', 'Fiber optic', 'No'], 10),\n",
    "    'Churn': np.random.randint(0, 2, 10) # 0 for No, 1 for Yes\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define features (X)\n",
    "feature_columns = ['Tenure', 'MonthlyCharges', 'TotalCharges', 'ContractType', 'InternetService']\n",
    "X = df[feature_columns]      # It selects only the columns listed in feature_columns from the DataFrame df\n",
    "\n",
    "print(\"Feature Matrix (X):\")\n",
    "print(X.head())\n",
    "print(\"Shape of X:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bfe618-458f-45b4-91bd-95de6a011f3d",
   "metadata": {},
   "source": [
    "The target vector, y, is a 1-dimensional array-like structure (typically a NumPy array or a Pandas Series) that contains the values or labels you are trying to predict. Each element in y corresponds to the target value for the observation at the same index in the feature matrix X.\n",
    "\n",
    "##### Why is it important?\n",
    "\n",
    "The target vector is the 'answer' that your model aims to learn. During training (the fit process), the model uses X to predict values and compares these predictions to the actual values in y to adjust its internal parameters. For supervised learning tasks, y is essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea219ed8-e71e-4b48-b2e3-81aaf8e351be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Vector (y):\n",
      "0    1\n",
      "1    1\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: Churn, dtype: int32\n",
      "Shape of y: (10,)\n"
     ]
    }
   ],
   "source": [
    "target_column = 'Churn'\n",
    "y = df[target_column]\n",
    "\n",
    "print(\"Target Vector (y):\")\n",
    "print(y.head())\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5114af1-e467-4398-bff8-541bceb122ea",
   "metadata": {},
   "source": [
    "### The Relationship Between X and y\n",
    "It is crucial that the number of rows in X (number of samples) exactly matches the number of elements in y (number of target values). If they do not match, Scikit-learn will raise an error. This ensures that each feature set has a corresponding correct answer for the model to learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce80cdd-37aa-4798-8178-3eb27a50ce55",
   "metadata": {},
   "source": [
    "# Splitting Data into Training and Testing Sets with train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c2d7bb-aef9-470e-b1bc-30b858c5635b",
   "metadata": {},
   "source": [
    "The training set is used to 'teach' the model. The model learns patterns, relationships, and parameters from this data. The testing set, on the other hand, is held back and used only after the model has been trained. It serves as a proxy for real-world, unseen data, allowing us to get an unbiased estimate of the model's performance.\n",
    "\n",
    "**Preventing Overfitting**: This is the primary reason. A model that performs perfectly on training data but poorly on test data is overfitted.       \n",
    "**Unbiased Evaluation**: The test set provides an honest assessment of how the model is likely to perform in production.                              \n",
    "**Model Selection**: When comparing different models or different hyperparameter settings for the same model, the performance on the test set helps you choose the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc31279-a0ed-4eb6-aec9-7800f6e1112d",
   "metadata": {},
   "source": [
    "**Syntax** :- X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "**X**: Feature matrix (input data) — Pandas DataFrame or NumPy array                                                                          \n",
    "**y**: Target vector (output/label) — Pandas Series or NumPy array                                                                                \n",
    "**test_size**: Fraction or number of samples for the test set (e.g., 0.2 = 20%). Remaining data is used for training                                \n",
    "**random_state**: Fixes randomness for reproducible splits (e.g., 42)                                                                               \n",
    "**shuffle (default: True)**: Shuffles data before splitting to ensure randomness                                                                      \n",
    "**stratify (optional)**: Maintains the same class distribution in train and test sets (use stratify=y for classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a7b7019-e13d-40bf-a708-e45c3465732a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- After Splitting ---\n",
      "Shape of X_train: (80, 3)\n",
      "Shape of X_test: (20, 3)\n",
      "Shape of y_train: (80,)\n",
      "Shape of y_test: (20,)\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'FeatureA': np.random.rand(100) * 10,\n",
    "    'FeatureB': np.random.rand(100) * 5,\n",
    "    'FeatureC': np.random.randint(0, 2, 100), # Example of a categorical-like feature\n",
    "    'Target': np.random.rand(100) * 100 # A continuous target for regression\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "feature_column = ['FeatureA', 'FeatureB', 'FeatureC']\n",
    "X = df[feature_column]\n",
    "y = df['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "print(\"--- After Splitting ---\")\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d7e2d-9afc-4c63-ab56-1e4971b2351c",
   "metadata": {},
   "source": [
    "## Standard Scikit-learn Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fb4484-237e-4aaa-8993-33080aa33fa9",
   "metadata": {},
   "source": [
    "1. **Import Libraries**  - Import required Python libraries such as NumPy, Pandas, and Scikit-learn.\n",
    "\n",
    "2. **Load Data**  - Load the dataset into a Pandas DataFrame.\n",
    "\n",
    "3. **Data Preprocessing & Feature Engineering**  \n",
    "   - Handle missing values (imputation)  \n",
    "   - Encode categorical variables (e.g., One-Hot Encoding)  \n",
    "   - Scale numerical features (Standardization / Normalization)  \n",
    "   - Create new features if required  \n",
    "\n",
    "4. **Define Features and Target**  - Separate the dataset into feature matrix **X** and target vector **y**.\n",
    "\n",
    "5. **Split the Data**  -  Use `train_test_split` to divide data into training and testing sets.\n",
    "\n",
    "6. **Model Selection**  -  Choose and instantiate a suitable machine learning model.\n",
    "\n",
    "7. **Train the Model**  -  Fit the model using the training data.\n",
    "\n",
    "8. **Make Predictions**  -  Predict outputs for the test data.\n",
    "\n",
    "9. **Evaluate the Model**  -  Evaluate model performance using appropriate metrics.\n",
    "\n",
    "10. **Iterate and Improve**  - Tune hyperparameters, try different models, or improve feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ff01a-70b7-4bb9-8c27-1839d9977ba6",
   "metadata": {},
   "source": [
    "# Tools for Your ML Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d328ba-c3db-476e-b62c-216205337972",
   "metadata": {},
   "source": [
    "### 1. The linear_model Module: Algorithms for Linear Relationships                                                                      \n",
    "The linear_model module contains a variety of algorithms that assume a linear relationship between the input features and the target variable. These are often some of the simplest yet most powerful models, serving as excellent baselines.                                                              \n",
    "\n",
    "##### Key Algorithms and Use Cases\n",
    "\n",
    "- **LinearRegression**  \n",
    "  Used for regression problems with a continuous target variable.  \n",
    "  Fits the best straight line (or hyperplane) to the data.\n",
    "\n",
    "- **Ridge Regression**  \n",
    "  Linear regression with **L2 regularization**.  \n",
    "  Reduces overfitting by shrinking coefficients, useful when features are highly correlated.\n",
    "\n",
    "- **Lasso Regression**  \n",
    "  Linear regression with **L1 regularization**.  \n",
    "  Can reduce some coefficients exactly to zero, performing **feature selection**.\n",
    "\n",
    "- **ElasticNet**  \n",
    "  Combines **L1 (Lasso)** and **L2 (Ridge)** regularization.  \n",
    "  Useful when multiple features are correlated and feature selection is needed.\n",
    "\n",
    "- **LogisticRegression**  \n",
    "  Used for **classification**, mainly binary classification.  \n",
    "  Models the probability of class membership using a logistic (sigmoid) function.\n",
    "\n",
    "- **SGDClassifier / SGDRegressor**  \n",
    "  Linear models trained using **Stochastic Gradient Descent (SGD)**.  \n",
    "  Efficient for **large-scale datasets** and online learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18583a6b-24b4-4152-904c-dd372f6237cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Coefficients: [44.43716999]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "\n",
    "X_reg, y_reg = make_regression(n_samples=100,n_features=1,noise=10,random_state=42)\n",
    "reg_model = LinearRegression()\n",
    "reg_model.fit(X_reg, y_reg)\n",
    "print(f\"Linear Regression Coefficients: {reg_model.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e6d1f9c-d0e9-4432-99db-593c7a949cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Coefficients: [[ 3.23216767 -0.84594518]]\n",
      "Logistic Regression Intercept: [0.05126339]\n"
     ]
    }
   ],
   "source": [
    "X_clf, y_clf = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "clf_model = LogisticRegression()\n",
    "clf_model.fit(X_clf, y_clf)\n",
    "print(f\"Logistic Regression Coefficients: {clf_model.coef_}\")\n",
    "print(f\"Logistic Regression Intercept: {clf_model.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ccc4d-1b45-435f-8057-3c7cc40bf7c3",
   "metadata": {},
   "source": [
    "### 2. The metrics Module: Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d161ab9-ecdd-44da-b12c-7d3d88fc99a8",
   "metadata": {},
   "source": [
    "The metrics module provides a comprehensive set of functions for evaluating the performance of machine learning models. These metrics help you understand how well your model is performing and identify areas for improvement.                                                                     \n",
    "\n",
    "#### Key Evaluation Metrics and Use Cases\n",
    "The choice of evaluation metrics depends on whether the problem is **regression** or **classification**.\n",
    "\n",
    "### Regression Metrics\n",
    "\n",
    "- **Mean Squared Error (MSE)**  \n",
    "  Measures the average squared difference between predicted and actual values.  \n",
    "  Penalizes larger errors more heavily.\n",
    "\n",
    "- **Mean Absolute Error (MAE)**  \n",
    "  Measures the average absolute difference between predicted and actual values.  \n",
    "  Less sensitive to outliers compared to MSE.\n",
    "\n",
    "- **R² Score (r2_score)**  \n",
    "  Indicates the proportion of variance in the target variable explained by the model.  \n",
    "  A value of `1` represents a perfect fit.\n",
    "\n",
    "### Classification Metrics\n",
    "\n",
    "- **Accuracy Score**  \n",
    "  Proportion of correctly classified samples.  \n",
    "  Best suited for balanced datasets.\n",
    "\n",
    "- **Precision Score**  \n",
    "  Ratio of true positives to all predicted positives:  \n",
    "  `TP / (TP + FP)`  \n",
    "  Measures correctness of positive predictions.\n",
    "\n",
    "- **Recall Score**  \n",
    "  Ratio of true positives to all actual positives:  \n",
    "  `TP / (TP + FN)`  \n",
    "  Measures the model’s ability to identify positive cases.\n",
    "\n",
    "- **F1 Score**  \n",
    "  Harmonic mean of precision and recall.  \n",
    "  Useful when class distribution is imbalanced.\n",
    "\n",
    "- **Confusion Matrix**  \n",
    "  Displays counts of true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5895d02-fda1-4f92-a28e-0818bb242096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Regression Metrics ---\n",
      "Mean Squared Error: 104.20\n",
      "R-squared: 0.94\n",
      "--- Classification Metrics ---\n",
      "Accuracy: 0.95\n",
      "Confusion Matrix: [[10  1]\n",
      " [ 0  9]]\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        11\n",
      "           1       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.95      0.95      0.95        20\n",
      "weighted avg       0.96      0.95      0.95        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "\n",
    "# Regression Evaluation\n",
    "X_reg, y_reg = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "reg_model = LinearRegression()\n",
    "reg_model.fit(X_reg_train, y_reg_train)\n",
    "y_reg_pred = reg_model.predict(X_reg_test)\n",
    "\n",
    "mse = mean_squared_error(y_reg_test, y_reg_pred)\n",
    "r2 = r2_score(y_reg_test, y_reg_pred)\n",
    "print(f\"--- Regression Metrics ---\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n",
    "\n",
    "# Classification Evaluation\n",
    "X_clf, y_clf = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)\n",
    "clf_model = LogisticRegression(random_state=42)\n",
    "clf_model.fit(X_clf_train, y_clf_train)\n",
    "y_clf_pred = clf_model.predict(X_clf_test)\n",
    "\n",
    "accuracy = accuracy_score(y_clf_test, y_clf_pred)\n",
    "conf_matrix = confusion_matrix(y_clf_test, y_clf_pred)\n",
    "class_report = classification_report(y_clf_test, y_clf_pred)\n",
    "\n",
    "print(f\"--- Classification Metrics ---\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\", conf_matrix)\n",
    "print(\"Classification Report:\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb621d-0222-4a77-b1f0-ed773052a7f6",
   "metadata": {},
   "source": [
    "#### 3. The model_selection Module: Managing Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207cb16-bf27-4786-a18f-d4a2f59dbb9c",
   "metadata": {},
   "source": [
    "The model_selection module provides tools for selecting the best models and tuning their hyperparameters. It's essential for robust model development and evaluation.                                                                                                                                      \n",
    "\n",
    "#### Key Functions and Use Cases\n",
    "\n",
    "- **train_test_split**  \n",
    "  Splits the dataset into training and testing sets to evaluate model performance on unseen data.\n",
    "\n",
    "- **KFold / StratifiedKFold**  \n",
    "  Used for **k-fold cross-validation**, where data is divided into multiple folds and the model is trained and tested multiple times.  \n",
    "  - `KFold`: Suitable for regression or balanced datasets  \n",
    "  - `StratifiedKFold`: Preserves class distribution, ideal for classification problems\n",
    "\n",
    "- **GridSearchCV**  \n",
    "  Performs **exhaustive hyperparameter tuning** by evaluating all specified parameter combinations using cross-validation.\n",
    "\n",
    "- **RandomizedSearchCV**  \n",
    "  Performs **randomized hyperparameter tuning** by sampling a fixed number of parameter combinations.  \n",
    "  More efficient than GridSearchCV for large search spaces.\n",
    "\n",
    "- **cross_val_score**  \n",
    "  Computes cross-validation scores for a model in a simple and efficient way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb211055-7dd2-43be-b2ac-8422e299bca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cross-Validation Scores ---\n",
      "Individual fold scores: [0.95 1.   1.   1.   0.95]\n",
      "Mean CV accuracy: 0.98\n",
      "Standard deviation of CV scores: 0.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate data\n",
    "X_cv, y_cv = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Instantiate a model\n",
    "model_cv = LogisticRegression(random_state=42)\n",
    "\n",
    "# Define K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42) # 5 folds\n",
    "\n",
    "# Compute cross-validation scores\n",
    "# This will train and evaluate the model 5 times\n",
    "cv_scores = cross_val_score(model_cv, X_cv, y_cv, cv=kf, scoring='accuracy')\n",
    "\n",
    "print(f\"--- Cross-Validation Scores ---\")\n",
    "print(f\"Individual fold scores: {cv_scores}\")\n",
    "print(f\"Mean CV accuracy: {cv_scores.mean():.2f}\")\n",
    "print(f\"Standard deviation of CV scores: {cv_scores.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f598f3-0612-4b85-ae36-65b4d9420789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
