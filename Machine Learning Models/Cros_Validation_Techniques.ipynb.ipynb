{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ec193f-b8ff-4761-bfef-c0f6cb822c1c",
   "metadata": {},
   "source": [
    "## 1. K-Fold Cross-Validation: The Workhorse of Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f5ee9b-ee08-4ce5-a32b-b409ada934f8",
   "metadata": {},
   "source": [
    "The dataset is divided into **k equal-sized subsets (folds)**. The model is trained and evaluated **k times**, each time using a different fold as the validation set.\n",
    "\n",
    "For each of the **k iterations**:\n",
    "\n",
    "1. One fold is kept aside as the **testing (validation) set**\n",
    "2. The remaining **kâˆ’1 folds** are combined to form the **training set**\n",
    "3. The model is trained on the training set\n",
    "4. The model is evaluated on the held-out fold\n",
    "5. The performance metric (Accuracy, MSE, RÂ², etc.) is recorded\n",
    "\n",
    "- Each fold is used **exactly once** as the validation set\n",
    "- Final performance = **Average of all k recorded scores**\n",
    "- This reduces bias caused by a single random train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf576c-a4cb-4ec3-997c-d13727ae4ebb",
   "metadata": {},
   "source": [
    "### Choosing the Value of `k` in K-Fold Cross-Validation\n",
    "The selection of `k` involves a **trade-off between reliability and computation cost**.\n",
    "\n",
    "#### ðŸ”º Higher Value of `k`\n",
    "- More reliable performance estimate\n",
    "- Model is trained **k times** â†’ higher computation cost\n",
    "- Training sets are larger and very similar\n",
    "- Can increase variance if folds are not well-represented\n",
    "\n",
    "#### ðŸ”» Lower Value of `k`\n",
    "- Faster computation\n",
    "- Less reliable performance estimate\n",
    "- Smaller and more distinct training sets\n",
    "- Can introduce higher bias in performance estimation\n",
    "\n",
    "#### Commonly Used Values\n",
    "- **k = 5** or **k = 10**\n",
    "  - Good balance between accuracy and computation time\n",
    "  - Widely used in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71acef01-17de-46ea-a1dc-77944bd53222",
   "metadata": {},
   "source": [
    "## 2. Stratified K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57050451-0946-446c-9910-d1741900660d",
   "metadata": {},
   "source": [
    "Stratified K-Fold Cross-Validation is a variation of standard K-Fold Cross-Validation designed specifically for **classification problems**, especially when the dataset is **imbalanced**.  \n",
    "Its main objective is to ensure that **each fold preserves the same class distribution** as the original dataset.\n",
    "Each fold contains approximately the same proportion of samples from each target class as found in the full dataset.\n",
    "\n",
    "### How Stratified K-Fold Works\n",
    "\n",
    "1. **Stratification**\n",
    "   - The dataset is first grouped based on the target variable (class labels).\n",
    "\n",
    "2. **Proportional Distribution**\n",
    "   - Samples from each class are distributed evenly across all `k` folds.\n",
    "   - Example:\n",
    "     - Original dataset: 80% Class A, 20% Class B  \n",
    "     - Each fold: approximately 80% Class A and 20% Class B\n",
    "\n",
    "3. **Iterative Evaluation**\n",
    "   - The process runs for `k` iterations:\n",
    "     - One fold is used as the validation (test) set.\n",
    "     - The remaining `kâˆ’1` folds are combined to form the training set.\n",
    "   - Both training and validation sets maintain the original class proportions.\n",
    "\n",
    "4. **Performance Averaging**\n",
    "   - Performance metrics (accuracy, precision, recall, F1-score, etc.) are recorded for each iteration.\n",
    "   - Final performance is computed as the **average across all folds**.\n",
    "\n",
    "### Why Stratified K-Fold Is Important\n",
    "- **Fair and Reliable Evaluation**\n",
    "  - Each fold represents the overall dataset distribution accurately.\n",
    "- **Improved Learning for Minority Classes**\n",
    "  - Minority classes are included in every training and validation split.\n",
    "- **Reduced Variance in Performance Scores**\n",
    "  - Leads to more stable and consistent evaluation results across folds.\n",
    "\n",
    "### When to Use Stratified K-Fold\n",
    "- Classification problems\n",
    "- Imbalanced datasets\n",
    "- When reliable evaluation of all classes is required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe443e4-ef33-4dad-acd4-8508296868a5",
   "metadata": {},
   "source": [
    "## 3. Leave-One-Out Cross-Validation (LOOCV): The Extreme Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab2c644-f5dc-42bd-9ce0-73ea7b0190fd",
   "metadata": {},
   "source": [
    "Leave-One-Out Cross-Validation (LOOCV) is a special case of K-Fold Cross-Validation where the number of folds `k` is equal to the number of data points `n` in the dataset.  \n",
    "In each iteration, exactly **one data point** is used as the test set, and the remaining `nâˆ’1` data points are used for training.\n",
    "\n",
    "LOOCV follows the steps below:\n",
    "\n",
    "For a dataset with `n` data points:\n",
    "1. Select one data point as the testing set.\n",
    "2. Use the remaining `nâˆ’1` data points as the training set.\n",
    "3. Train the model on the training set.\n",
    "4. Evaluate the model on the single held-out data point and record the performance metric.\n",
    "5. Repeat this process for all `n` data points.\n",
    "\n",
    "After all iterations:\n",
    "- A total of `n` performance scores are obtained.\n",
    "- The final performance estimate is the **average of these `n` scores**.\n",
    "\n",
    "### Why LOOCV is Important\n",
    "**Very Low Bias**  \n",
    "Since almost the entire dataset (`nâˆ’1` samples) is used for training in each iteration, the trained model closely resembles a model trained on the full dataset. This leads to a very low-bias performance estimate.\n",
    "\n",
    "**Deterministic Evaluation**  \n",
    "LOOCV produces the same result every time it is run on the same dataset, as there is only one possible way to leave out each data point.\n",
    "\n",
    "**Effective for Small Datasets**  \n",
    "When the dataset is very small, LOOCV allows maximum use of available data for training in each iteration.\n",
    "\n",
    "### When to Consider LOOCV\n",
    "\n",
    "LOOCV is generally used in limited scenarios due to its high computational cost:\n",
    "\n",
    "- Very small datasets where training `n` models is feasible\n",
    "- Theoretical analysis of model behavior and stability\n",
    "- Certain linear models where LOOCV computation can be optimized\n",
    "\n",
    "### Comparison with K-Fold Cross-Validation\n",
    "\n",
    "LOOCV can be viewed as K-Fold Cross-Validation with `k = n`, but with important trade-offs:\n",
    "\n",
    "- **Bias:** Lower bias than K-Fold (for `k < n`)\n",
    "- **Variance:** Often higher variance than K-Fold\n",
    "- **Computation:** Significantly more computationally expensive than K-Fold (e.g., `k = 5` or `k = 10`)\n",
    "\n",
    "**Practical Recommendation**: In most real-world applications, **K-Fold Cross-Validation with `k = 5` or `k = 10`** is preferred over LOOCV due to its better balance between computational efficiency, bias, and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf6fcd-72b2-46c2-a26d-92f8e1991be7",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation for a Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f491cd-1998-4997-91db-2244ad74b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c04c99b-9fdc-4c17-80c2-a68e75cd2a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic regression dataset\n",
    "np.random.seed(42)\n",
    "X,y = make_regression(n_samples=100,n_features=10,noise=10,random_state=42)\n",
    "x_df = pd.DataFrame(X, columns=[f'feature_{i}'for i in range(X.shape[1])])\n",
    "y_df = pd.Series(y,name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a63b7c0-2190-464f-87b6-b6a330da0056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing K-Fold Cross-Validation...\n",
      "--- Fold 1 ---\n",
      "MSE for Fold 1: 102.66\n",
      "--- Fold 2 ---\n",
      "MSE for Fold 2: 110.75\n",
      "--- Fold 3 ---\n",
      "MSE for Fold 3: 100.54\n",
      "--- Fold 4 ---\n",
      "MSE for Fold 4: 106.44\n",
      "--- Fold 5 ---\n",
      "MSE for Fold 5: 146.99\n",
      "--- Cross-Validation Results ---\n",
      "Average MSE: 113.48\n",
      "Standard Deviation of MSE: 17.11\n"
     ]
    }
   ],
   "source": [
    "# Initialize K-Fold\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=42)\n",
    "\n",
    "# Perform K-Fold Cross-Validation manually\n",
    "mse_scores=[]\n",
    "print('Performing K-Fold Cross-Validation...')\n",
    "\n",
    "for fold,(train_index,test_index) in enumerate(kf.split(x_df)):\n",
    "    print(f'--- Fold {fold+1} ---')\n",
    "    \n",
    "    # Split data into training and testing sets for this fold\n",
    "    X_train, X_test = x_df.iloc[train_index], x_df.iloc[test_index]\n",
    "    y_train, y_test = y_df.iloc[train_index], y_df.iloc[test_index]\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate Mean Squared Error for this fold\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "    print(f'MSE for Fold {fold+1}: {mse:.2f}')\n",
    "\n",
    "# Calculate the average MSE across all folds\n",
    "mean_mse = np.mean(mse_scores)\n",
    "std_mse = np.std(mse_scores)\n",
    "\n",
    "print('--- Cross-Validation Results ---')\n",
    "print(f'Average MSE: {mean_mse:.2f}')\n",
    "print(f'Standard Deviation of MSE: {std_mse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbfc902d-562a-48d7-8a43-57238325ff34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Using cross_val_score ---\n",
      "Scores for each fold: [102.65673458 110.74775381 100.54424578 106.43890505 146.98911372]\n",
      "Average MSE (cross_val_score): 113.48\n",
      "Standard Deviation of MSE (cross_val_score): 17.11\n"
     ]
    }
   ],
   "source": [
    "# Generating k-fold using in-built\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = LinearRegression()\n",
    "\n",
    "# scoring='neg_mean_squared_error' because cross_val_score maximizes the score\n",
    "# We will negate it later to get positive MSE\n",
    "scores = cross_val_score(model, x_df, y_df, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "print('--- Using cross_val_score ---')\n",
    "print(f'Scores for each fold: {np.negative(scores)}') # Negate to get positive MSE\n",
    "\n",
    "mean_cv_mse = np.mean(np.negative(scores))\n",
    "std_cv_mse = np.std(np.negative(scores))\n",
    "\n",
    "print(f'Average MSE (cross_val_score): {mean_cv_mse:.2f}')\n",
    "print(f'Standard Deviation of MSE (cross_val_score): {std_cv_mse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea07194-36f0-421d-b527-5b782174f58c",
   "metadata": {},
   "source": [
    "## Stratified K-Fold for a Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cafbfa79-fff3-4bd2-b84e-a8e53a930400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "537e308b-4349-4abc-a506-7cc20fbdc7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: 0    85\n",
      "1    15\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X_clf, y_clf = make_classification(n_samples=100, n_features=20, n_informative=10, n_redundant=5,\n",
    "                                   n_classes=2, n_clusters_per_class=2, weights=[0.9, 0.1], flip_y=0.05, random_state=42)\n",
    "X_clf_df = pd.DataFrame(X_clf)\n",
    "y_clf_df = pd.Series(y_clf)\n",
    "\n",
    "print(f'Class distribution: {pd.Series(y_clf).value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f189376a-209b-4aea-b807-078eb095b3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stratified K-Fold Results (Classification) ---\n",
      "Accuracy scores for each fold: [0.6  0.75 0.7  0.85 0.9 ]\n",
      "Average Accuracy: 0.760 +/- 0.107\n",
      "F1-scores (weighted) for each fold: [0.6375     0.76406926 0.7        0.78108108 0.87777778]\n",
      "Average F1-score (weighted): 0.752 +/- 0.081\n"
     ]
    }
   ],
   "source": [
    "# Initialize Stratified K Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# Perform Stratified K-Fold Cross-Validation using cross_val_score\n",
    "model_clf = LogisticRegression(solver='liblinear', random_state=42) # liblinear is good for small datasets\n",
    "\n",
    "# Evaluate using accuracy\n",
    "accuracy_scores = cross_val_score(model_clf, X_clf_df, y_clf_df, cv=skf, scoring='accuracy')\n",
    "\n",
    "# Evaluate using F1-score (weighted average is often good for imbalanced data)\n",
    "# 'weighted' accounts for label imbalance when computing the metric\n",
    "f1_scores = cross_val_score(model_clf, X_clf_df, y_clf_df, cv=skf, scoring='f1_weighted')\n",
    "\n",
    "print('--- Stratified K-Fold Results (Classification) ---')\n",
    "print(f'Accuracy scores for each fold: {accuracy_scores}')\n",
    "print(f'Average Accuracy: {np.mean(accuracy_scores):.3f} +/- {np.std(accuracy_scores):.3f}')\n",
    "\n",
    "print(f'F1-scores (weighted) for each fold: {f1_scores}')\n",
    "print(f'Average F1-score (weighted): {np.mean(f1_scores):.3f} +/- {np.std(f1_scores):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457cca0-170f-4fee-a810-11f42b4f90c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
